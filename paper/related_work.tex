\section{Related works} \label{sec:related_works}

% A few works have focused on segmentation of ASR data. \citet{bouchekif2015diachronic} and \citet{chifu2016segchain} developed methods for topic segmentation of TV broadcast news based on semantic features. \citet{purver2006unsupervised} and \citet{hsueh2010combining} focused on segmenting ASR transcripts of multiple dialogues or conversations. More recently, \citet{sehikh2017topic} has used recurrent neural networks (RNNs) for this task.

% The RNNs have been proven to be very successful with the sequence labeling task in recent years \cite{lipton2015critical}. In particular, the Bi-LSTM architecture has achieved good performance \cite{ma2016end, huang2015bidirectional} while allowing for end-to-end learning. This is made possible by learning hidden representations of input sequences \cite{sundermeyer2012lstm}, which encode contextual information for each token. More recently, based on the attention mechanism \cite{vaswani2017attention}, the BERT model \cite{devlin2018bert} and its variations have achieved the SOTA on many NLP tasks relevant to the sequence labeling task (e.g. SQuaD \cite{rajpurkar2016squad}). 

% Word embeddings have been widely used in NLP tasks. They can work as a stand-along model, or integrate into other NLP models naturally and improve their performance \cite{alemi2015text, naili2017comparative, song2016dialogue}. They also commonly serve as foundations of more complex architectures such as the RNNs.

A general layout of discourse theory is provided in \citet{grimes1972thread}. \citet{mann1988rhetorical} laid the foundation of Rhetorical Structure Theory (RST), which includes a notion of higher-level ``structures" similar to what we consider. Multiple theories and grammars of discourse structure have been proposed, e.g. \citet{grosz1986attention, polanyi1983syntax, polanyi1996linguistic, asher2003logics}. In NLP, the RST Discourse Treebank corpus has been the foundation of many works devoted to segmenting a text into elementary discourse units (EDUs) and annotating such units (e.g. see \citet{ferracane2019news}). Recently, RNN and attention models have also been used to build discourse-aware models for segmentation and summarization tasks \cite{wangetal2018toward, cohan2018discourse, xu2019discourse}. While some of these works involve segmentation based on discourse structure, they focus on EDUs or small, clausal units, rather than high-level structures.
 
While no existing work has addressed the problem of identifying macro-level discourse structure to the best of our knowledge, our tasks are similar to the other text segmentation tasks in NLP, where smaller parts of a text coherent in some sense are extracted \cite{pak2018text}. In the NLP literature, the majority of text segmentation focus on topic-based segmentation---extracting topically-coherent contiguous blocks from a longer text \cite{li2018segbot}. Many methods for topic segmentation have been well-developed using lexical or semantic features \cite{hearst1997texttiling, choi2000advances}, ontologies \cite{bayomi2015ontoseg}, and more recently RNNs. \citet{badjatiya2018attention} and \citet{li2018segbot} used bi-directional RNNs for general text segmentation. \citet{sehikh2017topic} also used Bi-RNNs for topic segmentation. \citet{salloum2017automated} used a Bi-LSTM model to segment the preamble from the body text in medical transcripts. However, to our knowledge, BERT has not been applied to the general text segmentation task, and no RNN or BERT models have been developed for text segmentation relating to discourse structure.

The explanation of deep neural network models has attracted much recent interest. In NLP, a recent method called edge probing \cite{tenney2019you} has been used to assess the information stored in specific layers. Applying this method on BERT has shown that BERT is able to learn classical linguistic information \cite{jawahar2019does}. Other research has analyzed BERT's attention mechanism and attempted to explain its behavior \cite{van2019does, clark2019does, wiegreffe2019attention, voita2019analyzing, jain2019attention}.



