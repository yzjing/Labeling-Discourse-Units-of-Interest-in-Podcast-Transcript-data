\section{Discussion}\label{sec:discussion}

%% New (monday):

In this work, we have developed an approach to segment text data based on macro-level discourse structure using sequence models. While both of our models---Bi-LSTM and BERT---perform very well on the NeurIPS task, it is much harder to achieve high accuracy on the Podcast task. We suspect that the NeurIPS task is simpler due to the highly regular rhetorical structure of scientific paper abstracts. A good abstract mirrors the construction of a paper, as currently construed in scientific literature. The abstract will begin with an outlay of the field of study, the questions to be addressed, mentions of related work, then continues with a brief identification of methods and data, and ends with a statement of conclusions and results. 

Perhaps because the structure of this type of discourse is so fixed, the transition from the abstract to the longer introduction within the paper itself is easier to identify. In the Podcast task, the structure of the introduction segment is both less fixed by convention, and more variable across different podcast genres. Although we did find that some programs had very regular introduction structure (e.g. {\it Song Exploder}), many had a looser, more conversational style, without a standardized structure.

Transcription noise is also a factor in the podcast dataset, as is the fundamental difference between written and speech data. While the written NeurIPS texts are well-edited, the speech data is generally less organized, and also error-prone, with restarts, disfluencies, and interruptions. All of these may be reasons why a complex model like BERT is needed to capture the structures in the podcast data.

% This irregular nature of the captured data poses more challenges for future work related to spoken discourse.

On the Podcast task, while the fine-tuned BERT model and Bi-LSTM model have similar performance on the test set of seen programs, the BERT model significantly out-performs the Bi-LSTM model on the test set of unseen programs. This demonstrates BERT's ability to extend to unfamiliar data, as well as suggests the existence of some embedded linguistic and structural knowledge, which the model is able to generalize across different introduction structures. Such knowledge appears to be only identifiable to BERT, and not the simpler models. We speculate this is because BERT's multiple attention layers allow it to learn and maintain richer syntactic, semantic, and task-specific information. Although we are able to recognize some of this information captured in the model's attention architecture, we have yet to fully understand how the model is able to learn it. A future direction of our work is to provide further explanation of these models' behaviour.